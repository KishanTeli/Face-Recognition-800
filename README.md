# Face-Recognition-800

# CPE 800 Project
# Face Recognition Using Deep Learning
# Kishan Teli, Raveena Mehta, Prof. Kevin Lu
# Introduction:
Our project addresses the problem of the increasing digital security threats and issues faced. The idea of facial recognition is better than the other existing solutions for authentication, because it doesn’t require user’s active co-operation. All the other features such as fingerprints, passcodes, etc. used for authentication purposes, need an active co-operation from users. Thus, the problem of ease needs to be solved. In this scenario, face recognition plays an important role. Another problem that our project addresses is something that has risen in recent times, is that of COVID-19 pandemic. Due to this, people’s need to wear masks on their faces have increased, so as to reduce its spread and be safe themselves as well. Hence, our project recognizes faces even when they have worn masks.

There were many challenges in implementing this project. We tried to achieve our goal by using histogram of oriented gradients (HOG) for face detection and for training our system, we used deep convolutional neural network (CNN). We used linear support vector machine (SVM) model classifier as well for implementing our project. But we failed to achieve our main goal, i.e., to detect faces while wearing a mask. We could not do so by facial landmark estimation algorithm, as we were unable to map the remaining 34 points of the other half of the face. Within the time constraints, we also had to create our own database in order to recognize the faces. Hence, we researched more on alternatives through which our goal could be achieved. After reading papers and accessing the internet, we finally landed on a great alternative that gave us results. In order to detect faces with masks, we scrapped our previous solution of predicting faces and instead decided to train our system by RetinaNet pre-trained model. This model along with Xception model used for classification, trained on ImageNet database were implemented to reach our goal of the project. The other challenge that we faced while implementing the new approach, was that we needed a lot of collection of data, in this case, images of masked people, in order to train our system.

There are many existing works related to facial recognition. Facial recognition could be achieved by using multi-pose representations [2]. This was possible by using 3-D rendering to generate multi-face poses from any input image given. In the above-mentioned article, they discussed about the implementation of CNN models to achieve the same and how by using this method, they increased their performance more than that of the supervised learning such as gallery fine-tuning and metric learning. There is mention of recognizing faces when live video streaming is being done [5]. FaNC is a novel face normalized method that is used to reduce pose-induced image variance [6]. There was an interesting article that mentioned about the kinship synthesis framework used to predict a smile of a child from the faces of the smile of his/her parents [8]. We studied this article in more detail in

order to learn more about predicting faces. The paper discussed about implementing the dynamics and appearance model together, to generate synthesized smile video of the estimated child from the real smile videos of their parents. Their studies show that these synthesized videos that depicted the child’s smile had more accuracy than the real videos of their children’s smile. They concluded that by synthesizing a dataset, we could achieve high accuracy in facial recognition and prediction. Target face association (TFA) is used to retrieve a set of representative face images from a video that is likely to have a same identity as the target face [10]. We understood the reasons due to which RetinaNet matches the speed of one-stage detectors and also surpasses the accuracy of 2-stage detectors and got introduced to the concept of focal loss for reducing class imbalance [12]. The comparison between Xception and Inception were discussed in an article, on the basis of their accuracy and performance [13].

Our paper will discuss about the phases of our project: Face detection, Posing and projecting faces, Identification of a face using training by RetinaNet model and Xception model instead of using facial landmarks and lastly, Identifying the face from our dataset and displaying the name of the person. We will also detect whether a face is with mask or without mask and display the same on screen. Accuracy will be determined. Recognition should be accurate even if only half of the face is given as the data to our system.
# Problem Description:
Face recognition involves many complex tasks that needs to be divided into stages. We are not just going to recognize faces from a given photo or a live video stream, we will also predict a person's face, when only half of it is given to the system as the data. And then, we will match the predicted face with our database, in order to recognize the face and identify it. Thus, there are four main stages by which we can achieve face recognition using deep learning. Face detection is the first basic stage wherein, we generate histogram of oriented gradients (HOG) representation of the image and then, we find the part of this image that matches or is similar to the HOG pattern extracted from a number of other training faces. In the next stage, i.e., posing and projecting faces – we need to detect faces from images having different poses of the same person. Hence, in this, we will use face landmark estimation algorithm. In this, we will train machine learning algorithm to find 68 specific points on any face, to solve this issue. The third stage, which is the most critical stage, is encoding faces. In this, we will use a number of measurements unique to a person’s face, which will help us identify a face. We use deep convolutional neural network (CNN) for this. Thus, we will be able to identify a person’s face. We will also do all these above tasks manually first, and then will train the machine later. We will also map the facial points of half of the face to the predicted facial points of the other half of the face. After the mapping is successfully done, we then use it to identify the person in the third stage. The last stage is the easiest stage, in which we need to find the name of the person/face that we identified from the database and display it. We use linear SVM model classifier for this. We need to run this classifier on our photo or video, to get the output, i.e, the name of the face of the person identified in the video or image. These combined tasks, enable us to recognize faces of people with high precision.

Face recognition involves many complex tasks that needs to be divided into stages. Thus, there are four main stages by which we can achieve face recognition using deep learning. All the four stages are being implemented by two models. Face Mask Detection is done on the basis of RetinaNet model and a separate model used for classification called as Xception.

In order to achieve our objective, we need to first train these models. To do so, we need many datasets, which consists of images in which people wear masks. As our aim is to make a model that detects faces even when a mask is worn, we need to train our model with such images. This way, we make our model more precise. Hence, first step is, to collect as many datasets as possible. We also need to look for datasets that contain images wherein, there are a large number of people, i.e., dense. Once, we have the right datasets, we will start training our models. We will also read images and put data into them through data generator. It also allows parallel loading of the input bytes through shared memory.

As RetinaNet and Xception model, both are pre-trained models, by training them for our specific output, we increase their precision and recall performance metric further. After training our models sufficiently, we then check for output of our project. We will check for our desired output but also, for analysis of our model’s performances. We will measure their performance in terms of accuracy, precision, recall, loss and F1-score. F1-score is the harmonic mean of precision and recall. It is chosen as the criterion for evaluation for the classification model. Being bound between 0 and 1, F1-score reaches its best value at 1 and worst at 0.

The coronavirus was declared a pandemic by WHO on March 11th, 2020, and by that time, we already had 118,000 confirmed cases and had spread to 114 countries around the globe. Since then, due to a variety of reasons some of which include strict measures not being put into place and government negligence, we have seen these figures blown out of proportions. Currently, at the time of writing this, we have 16 million confirmed cases and near 645,000 deaths worldwide. While some factors can be eluded as being not being under our control as individuals, we cannot say that face masks do not have a big role to play in controlling this pandemic.

# Data Collection and Preprocessing
# 1. Data Collection for Training

The total number of images in the training set were divided into the two categories as followe:
  1. with mask : 1938 images
  2. without mask: 1930 images

# Model Fitting
  Batch size: 32
  Epochs: 2
  Learning rate : 1e-4
  Gradient Descent Optimizer : Adam
  Loss function : Sparse cross entropy
  Metric evolution : F1-score
 
 We checked all photos that include in our dataset that we get from online and we used some kaggle dataset include crowd based settiong of people. Applied RetinaNet face model for face detection to generate detected face crops from the input image.
 
# Results
  
  Results obtained on the classification model:
  Loss - 0.0182
  F1 score: 0.99

Code and outputs in our drive because of the size of code is bigger.

# Conclusion
We successfully implemented Mask Detection Model by using RetinaNet model because of its high precision and recall performance and as it uses focus loss. Xception model was implemented because it uses depthwise separable convolutions, giving better performance on ImageNet database. By combining the two, we were able to recognize faces from an image input file, in which people wore masks. It was able to detect faces with masks as well. Our model delivers better performance than CNN models or models that uses facial landmark algorithms for facial recognition. We can use this model during the pandemic crisis. We would like to use this model to predict a person’s face, as well, in the near future. We would like to create a solid general database that can identify famous personalities, celebrities, etc. in the future.
